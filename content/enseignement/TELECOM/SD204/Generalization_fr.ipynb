{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDI 720 : Statistiques\n",
    "## Generalization\n",
    "### *Joseph Salmon*\n",
    "\n",
    "This notebook reproduces the pictures for the courses \"Generalization_fr\" / \"Generalization_en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from os import mkdir, path\n",
    "import statsmodels.formula.api as smf\n",
    "from matplotlib import rc\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy.interpolate import UnivariateSpline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dirname = \"../prebuiltimages/\"\n",
    "if not path.exists(dirname):\n",
    "    mkdir(dirname)\n",
    "\n",
    "np.random.seed(seed=44)\n",
    "\n",
    "###############################################################################\n",
    "# Plot initialization\n",
    "\n",
    "plt.close('all')\n",
    "dirname = \"../srcimages/\"\n",
    "imageformat = '.pdf'\n",
    "\n",
    "\n",
    "rc('font', **{'family': 'sans-serif', 'sans-serif': ['Computer Modern Roman']})\n",
    "params = {'axes.labelsize': 12,\n",
    "          'font.size': 16,\n",
    "          'legend.fontsize': 16,\n",
    "          'text.usetex': True,\n",
    "          'figure.figsize': (8, 6)}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "sns.set_context(\"poster\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "sns.set_style(\"white\")\n",
    "sns.axes_style()\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# display function:\n",
    "\n",
    "saving = False\n",
    "\n",
    "\n",
    "def my_saving_display(fig, dirname, filename, imageformat):\n",
    "    \"\"\"\"Saving with personal function.\"\"\"\n",
    "    filename = filename.replace('.', 'pt')  # remove \".\" to avoid floats issues\n",
    "    if saving is True:\n",
    "        dirname + filename + imageformat\n",
    "        image_name = dirname + filename + imageformat\n",
    "        fig.savefig(image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivating example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed=42)\n",
    "n_samples = 60\n",
    "sigma = 0.2  # noise level\n",
    "x = np.sort(3. * np.random.rand(n_samples))\n",
    "xx = np.linspace(-.2, 3.2, 200)\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(x ** 2) + 2. + x\n",
    "\n",
    "\n",
    "y = f(x)\n",
    "y += sigma * np.random.randn(n_samples)\n",
    "\n",
    "# Ploting Signal / Noise\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.ylim([1.5, 7])\n",
    "plt.xlim([-.5, 3.5])\n",
    "plt.plot(xx, f(xx), '--k', label=\"true signal\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "my_saving_display(fig, dirname, \"sin_signal\", imageformat)\n",
    "plt.plot(x, y, 'o', label=\"noisy observations\")\n",
    "my_saving_display(fig, dirname, \"sin_signal_noisy\", imageformat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Performing OLS\n",
    "X = x[:, np.newaxis]\n",
    "ols = LinearRegression(fit_intercept=True)\n",
    "y_pred = ols.fit(X, y).predict(X)\n",
    "\n",
    "# Plotting Signal / OLS\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.plot(x, y, 'o', label=\"noisy observations\")\n",
    "plt.plot(X, y_pred, '-k', label=\"linear prediction\")\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.ylim([1.5, 7])\n",
    "plt.xlim([-.5, 3.5])\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "my_saving_display(fig, dirname, \"sin_signal_OLS\", imageformat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "degrees = [2, 3, 4, 5, 6]\n",
    "colors = [2, 3, 4, 5, 7]\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    X_poly = np.vander(x, degree + 1, increasing=True)\n",
    "    # Equivalent to:\n",
    "    # X_poly = np.column_stack([x ** i for i in range(degree + 1)])\n",
    "    ols = LinearRegression(fit_intercept=False)  # intercept already included!\n",
    "    ols.fit(X_poly, y)\n",
    "    y_pred = ols.predict(np.vander(xx, degree + 1, increasing=True))\n",
    "    print(\"OLS coef\")\n",
    "    print(ols.coef_) # not the small coef on dim 2\n",
    "    # Signal / OLS\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    plt.plot(x, y, 'o', label=\"noisy observations\")\n",
    "    plt.plot(xx, y_pred, '-',\n",
    "             color=sns.color_palette(\"colorblind\", 8)[colors[i] - 2],\n",
    "             label=r\"Polynomial prediction (deg={0})\".format(degree))\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$f(x)$')\n",
    "    plt.ylim([1.5, 7])\n",
    "    plt.xlim([-.5, 3.5])\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    my_saving_display(fig, dirname, \"sin_signal_poly\" + str(degree),\n",
    "                      imageformat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "degrees = [3, 4, 5, 6]\n",
    "colors = [3, 4, 5, 7]\n",
    "\n",
    "fig2 = plt.figure(figsize=(8, 6))\n",
    "plt.plot(x, y, 'o', label=\"noisy observations\")\n",
    "for i, degree in enumerate(degrees):\n",
    "    X_poly = np.vander(x, degree + 1, increasing=True)\n",
    "    ols = LinearRegression(fit_intercept=False)  # intercept already included!\n",
    "    ols.fit(X_poly, y)\n",
    "    y_pred = ols.predict(np.vander(xx, degree + 1, increasing=True))\n",
    "    plt.plot(xx, y_pred, '-',\n",
    "             color=sns.color_palette(\"colorblind\", 8)[colors[i] - 2],\n",
    "             label=r\"Polynomial prediction (deg={0})\".format(degree))\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$f(x)$')\n",
    "    plt.ylim([1.5, 7])\n",
    "    plt.xlim([-.5, 3.5])\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.tight_layout()\n",
    "plt.show()\n",
    "my_saving_display(fig2, dirname, \"sin_signal_poly_all\", imageformat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# defining a class for Polynonial Regression\n",
    "\n",
    "\n",
    "class PolynomialRegression(LinearRegression):\n",
    "    \"\"\"PolynomialRegression Class, to perform for instance CV\"\"\"\n",
    "\n",
    "    def __init__(self, degree=2):\n",
    "        super(PolynomialRegression, self).__init__()\n",
    "        self.degree = degree\n",
    "        self.fit_intercept = False\n",
    "\n",
    "    def fit(self, X, y, deg=None):\n",
    "        X = np.vander(X, N=self.degree + 1)\n",
    "        super(PolynomialRegression, self).fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.vander(X, N=self.degree + 1)\n",
    "        return super(PolynomialRegression, self).predict(X)\n",
    "\n",
    "\n",
    "model = PolynomialRegression(degree=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# On the importance of cross-validation\n",
    "\n",
    "estimator = PolynomialRegression()\n",
    "degrees = np.arange(2, 12)\n",
    "cv_model = GridSearchCV(estimator, cv=5,\n",
    "                        param_grid={'degree': degrees},\n",
    "                        scoring='neg_mean_squared_error')\n",
    "# Note: in scoring above, neg means -MSE\n",
    "cv_model.fit(x, y)\n",
    "\n",
    "mse = [-cv_score for cv_score in cv_model.cv_results_['mean_test_score']]\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.semilogy(degrees, mse, label=\"CV MSE curve\")\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('CV MSE')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "\n",
    "my_saving_display(fig2, dirname, \"CV_curve\", imageformat)\n",
    "\n",
    "best_idx = np.argmax(cv_model.cv_results_['mean_test_score'])\n",
    "best_deg = degrees[best_idx]\n",
    "best_mse = -cv_model.cv_results_['mean_test_score'][best_idx]\n",
    "plt.plot(best_deg, best_mse, \"*r\", markersize=40, label=\"Best degree (CV MSE)\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "\n",
    "my_saving_display(fig2, dirname, \"CV_curve_and_best\", imageformat)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# alternatively, onec can do it directly with sklearn:\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(degree=6), LinearRegression())\n",
    "y_pred = model.fit(x[:, np.newaxis], y).predict(xx[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SPLINE PART :\n",
    "Beware sicpy needs the points to be in ascending order!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Follow guidelines from:\n",
    "# http://danielhnyk.cz/creating-your-own-estimator-scikit-learn/\n",
    "# for creating an sklearn compatible class\n",
    "\n",
    "\n",
    "class UnivariateSplineSmoother(LinearRegression):\n",
    "    def __init__(self, s=4.):\n",
    "        self.s = s\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self._spline = UnivariateSpline(x, y, s=self.s)\n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self._spline(x)\n",
    "\n",
    "\n",
    "# large curvature means s is small\n",
    "spline_smoother = UnivariateSplineSmoother(0.5).fit(x, y)\n",
    "y_pred = spline_smoother.predict(xx)\n",
    "\n",
    "# low curvature means s is large\n",
    "spline_smoother = UnivariateSplineSmoother(2.).fit(x, y)\n",
    "y_pred_bis = spline_smoother.predict(xx)\n",
    "\n",
    "# Plotting Splines\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.plot(x, y, 'o', label=\"noisy observations\")\n",
    "plt.plot(xx, y_pred, '-', label=\"Spline: large curvature\")\n",
    "plt.plot(xx, y_pred_bis, '-', label=\"Spline: low curvature\")\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.ylim([1.5, 7])\n",
    "plt.xlim([-.5, 3.5])\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "my_saving_display(fig, dirname, \"splines\", imageformat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "estimator = UnivariateSplineSmoother()\n",
    "s_values = np.linspace(0.1, 10, num=20)\n",
    "cv_model_spl = GridSearchCV(estimator, cv=5,\n",
    "                            param_grid={'s': s_values},\n",
    "                            scoring='neg_mean_squared_error')\n",
    "# Note: in scoring above, \"neg_mean_squared_error\" means -MSE\n",
    "cv_model_spl.fit(x, y)\n",
    "\n",
    "mse_spl = [-cv_score for cv_score in cv_model_spl.cv_results_['mean_test_score']]\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.semilogy(s_values, mse_spl, label=\"CV MSE curve\")\n",
    "plt.xlabel('s values')\n",
    "plt.ylabel('CV MSE Splines')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "my_saving_display(fig, dirname, \"CV_curve_spline\", imageformat)\n",
    "\n",
    "\n",
    "best_idx_spline = np.argmax(cv_model_spl.cv_results_['mean_test_score'])\n",
    "best_s_spline = s_values[best_idx_spline]\n",
    "best_mse_spline = -cv_model_spl.cv_results_['mean_test_score'][best_idx_spline]\n",
    "plt.plot(best_s_spline, best_mse_spline, \"*r\", markersize=40,\n",
    "         label=\"Best s value (CV MSE)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "my_saving_display(fig, dirname, \"CV_curve_and_best_spline\", imageformat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# large curvature means s is large\n",
    "y_pred_bis = cv_model_spl.predict(xx)\n",
    "\n",
    "# Plotting Splines\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.plot(x, y, 'o', label=\"noisy observations\")\n",
    "plt.plot(xx, y_pred, '-', label=\"Spline: CV choice\")\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.ylim([1.5, 7])\n",
    "plt.xlim([-.5, 3.5])\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "my_saving_display(fig, dirname, \"best_spline_display\", imageformat)\n",
    "plt.show()\n",
    "\n",
    "print(\"MSE Polynomials = {0})\".format(best_mse))\n",
    "print(\"MSE Spline = {0})\".format(best_mse_spline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAM PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GeneralizedAdditiveRegressor(LinearRegression):\n",
    "    \"\"\"Fit Generalized Additive Model with backfitting\"\"\"\n",
    "\n",
    "    def __init__(self, smoothers, max_iter=20):\n",
    "        self.smoothers = smoothers\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.y_mean_ = np.mean(y)\n",
    "\n",
    "        residuals = y.copy()\n",
    "        residuals -= self.y_mean_\n",
    "        for i in range(self.max_iter):\n",
    "            for j in range(n_features):\n",
    "                if i > 0:\n",
    "                    residuals += self.smoothers[j].predict(X[:, j])\n",
    "\n",
    "                self.smoothers[j].fit(X[:, j], residuals)\n",
    "                residuals -= self.smoothers[j].predict(X[:, j])\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        y = np.ones(n_samples) * self.y_mean_\n",
    "        for j in range(n_features):\n",
    "            y += self.smoothers[j].predict(X[:, j])\n",
    "        return y\n",
    "\n",
    "\n",
    "def f1(x):\n",
    "    return np.cos(3 * x)\n",
    "\n",
    "\n",
    "def f2(x):\n",
    "    return x ** 3\n",
    "\n",
    "\n",
    "def f3(x):\n",
    "    return 3 * np.log(1 + np.abs(x))\n",
    "\n",
    "\n",
    "def f_true(X):\n",
    "    return f1(X[:, 0]) + f2(X[:, 1]) + f3(X[:, 2])\n",
    "\n",
    "\n",
    "n_samples = 100\n",
    "\n",
    "x1 = np.sort(2. * np.random.rand(n_samples) - 1.)\n",
    "x2 = np.sort(2. * np.random.rand(n_samples) - 1.)\n",
    "x3 = np.sort(2. * np.random.rand(n_samples) - 1.)\n",
    "\n",
    "X = np.c_[x1, x2, x3]\n",
    "y = f_true(X)\n",
    "y += 0.1 * np.random.randn(n_samples)\n",
    "\n",
    "\n",
    "s1 = 10 * np.var(y)\n",
    "s2 = 10 * np.var(y)\n",
    "s3 = 10 * np.var(y)\n",
    "smoothers = [UnivariateSplineSmoother(s=s1), UnivariateSplineSmoother(s=s2),\n",
    "             UnivariateSplineSmoother(s=s3)]\n",
    "gam = GeneralizedAdditiveRegressor(smoothers, max_iter=200)\n",
    "gam.fit(X, y)\n",
    "y_pred = gam.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx = np.linspace(-1, 1, 100)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 5),\n",
    "                         sharex=True, sharey=True)\n",
    "\n",
    "for j, (smoother, ax, f) in enumerate(zip(smoothers, axes.flat, [f1, f2, f3])):\n",
    "    f_j = smoother.predict(xx)\n",
    "    ax.plot(xx, f(xx) - np.mean(f(xx)), \"--\", label=\"true\")\n",
    "    ax.set_title(\"$x_{0}$\".format(j + 1))\n",
    "plt.ylim([-1.5, 1.5])\n",
    "plt.xlim([-1., 1])\n",
    "plt.tight_layout()\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "my_saving_display(fig, dirname, \"GAM-true\", imageformat)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 5),\n",
    "                         sharex=True, sharey=True)\n",
    "for j, (smoother, ax, f) in enumerate(zip(smoothers, axes.flat, [f1, f2, f3])):\n",
    "    f_j = smoother.predict(xx)\n",
    "    ax.plot(xx, f(xx) - np.mean(f(xx)), \"--\", label=\"true\")\n",
    "    ax.plot(xx, f_j - np.mean(f_j), label=\"prediction\")\n",
    "    plt.ylim([-1.5, 1.5])\n",
    "    plt.xlim([-1., 1])\n",
    "    ax.set_title(\"$x_{0}$\".format(j + 1))\n",
    "plt.tight_layout()\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "my_saving_display(fig, dirname, \"GAM-true-predict\", imageformat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Robustness PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed=42)\n",
    "n_samples = 60\n",
    "sigma = 0.2  # noise level\n",
    "x = np.sort(3. * np.random.rand(n_samples))\n",
    "xx = np.linspace(-.2, 3.2, 200)\n",
    "\n",
    "y = 3 * x\n",
    "espilon = 5 * np.random.randn(n_samples)\n",
    "espilon[1] = 70\n",
    "espilon[2] = 50\n",
    "espilon[3] = 45\n",
    "espilon[5] = 48\n",
    "\n",
    "y += espilon\n",
    "\n",
    "# Performing OLS\n",
    "X = x[:, np.newaxis]\n",
    "ols = LinearRegression(fit_intercept=True)\n",
    "y_pred = ols.fit(X, y).predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = {'temp': x, 'dens': y}\n",
    "df = pd.DataFrame(data=d)\n",
    "mod = smf.quantreg('dens ~ temp', df)\n",
    "res = mod.fit(q=.5)\n",
    "\n",
    "# Plotting Signal / OLS\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.plot(X, 3 * X, '--k', label=\"true\")\n",
    "plt.plot(X, y, 'o', label=\"noisy observations\")\n",
    "plt.plot(X, y_pred, '-', label=\"linear prediction\")\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.xlim([0, 3])\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "my_saving_display(fig, dirname, \"robust_ols\", imageformat)\n",
    "\n",
    "plt.plot(x, res.predict({'temp': x}), linestyle='-',\n",
    "         label='LAD statsmodel', zorder=2)\n",
    "plt.xlim([0, 3])\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "my_saving_display(fig, dirname, \"robust_lad\", imageformat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
